{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Simple Spark operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook requires a local installation of [Apache Spark](https://spark.apache.org/) (version 1.2 was used here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Load a text file and count the lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of sample text files (from Project Gutenberg http://www.gutenberg.org/) in the /data directory in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample data files (you could use any text file)\n",
    "shakespeare = \"./data/shakespeare-works.txt\"\n",
    "aurelius = \"./data/marcus-aurelius-meditations.txt\"\n",
    "\n",
    "# Load one of these files\n",
    "fileName = shakespeare\n",
    "\n",
    "textFile = sc.textFile(fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Count the lines in the file. \n",
    "* NB: This action will also force the RDD to materialise the data i.e. Spark will read the file at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x108a0f3d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ./data/shakespeare-works.txt contains 124787 lines\n"
     ]
    }
   ],
   "source": [
    "print(\"File {0} contains {1} lines\".format(fileName, textFile.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Takes the length of each line in the file and find the maximum line length.\n",
    "* Again, this action forces Spark to read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum line length is 85\n"
     ]
    }
   ],
   "source": [
    "maxlen = textFile.map(lambda s: len(s)).max()\n",
    "print(\"Maximum line length is {0}\".format(maxlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Create a simple Python function to count words in a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will use these functions when we process the Spark RDDs below.\n",
    "* The `extract_words()` function is pretty basic and we could probably refine this with a better regex, making it case-insensitive etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import regular expression library\n",
    "import re\n",
    "\n",
    "def extract_words(s):\n",
    "    \"\"\"Split input string into words using RE\"\"\"\n",
    "    return re.split('\\W+', s)\n",
    "\n",
    "def count_words(s):\n",
    "    \"\"\"Split input string into words using RE and return count\"\"\"\n",
    "    words = extract_words(s)\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the count_words function with a simple string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 'why hello there, Old old chap' contains 6 words\n"
     ]
    }
   ],
   "source": [
    "s = \"why hello there, Old old chap\"\n",
    "print(\"String '{0}' contains {1} words\".format(s, count_words(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Use your custom Python function with your Spark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this custom function in the standard RDD map() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsPerLine = textFile.map(lambda s: count_words(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words in a line is: 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum number of words in a line is: {0}\".format(wordsPerLine.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Do some classic word-count stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each line in the text file is read and converted into a list of words.\n",
    "* The word-lists for the separate lines are then *flattened* into a single list of words.\n",
    "* We then do the classic word-count **`map`** operation i.e. spit out a pair of (word, 1) for each word in the list.\n",
    "* Now we **`reduce`** the list of (word, 1) tuples, grouping them by the key (word), and adding up all the 1's for each word.\n",
    "* This gives as a **tuple of (word, count)** for each distinct word in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate word counts as another RDD\n",
    "countsRDD = textFile.flatMap(lambda line: extract_words(line.lower())) \\\n",
    "            .filter(lambda word: len(word)>0) \\\n",
    "            .map(lambda word: (word, 1)) \\\n",
    "            .reduceByKey(lambda a, b: a + b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We might choose to cache this RDD in Spark, so that we can re-use the RDD in different places later on without having to re-generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:42"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countsRDD.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find N most common words in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark likes working with [key-value pairs](http://spark.apache.org/docs/1.2.0/programming-guide.html#working-with-key-value-pairs) and our tuples are (key, value) pairs of **(word, count)**, i.e. the key is **word**.\n",
    "* But we now want to sort them in order of **count** to get the most/least common words.\n",
    "* The easy way to do this is to flip the tuples around as (count, word), then sort them by their key (count).\n",
    "* Then we just `take` the required number of items off the sorted list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find N most common words by count\n",
    "n = 10\n",
    "mostcommon = countsRDD \\\n",
    "           .map(lambda (w,c):(c,w)) \\\n",
    "           .sortByKey(ascending=False) \\\n",
    "           .take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 10 words\n",
      "====================\n",
      "the : 27843\n",
      "and : 26847\n",
      "i : 22538\n",
      "to : 19882\n",
      "of : 18307\n",
      "a : 14800\n",
      "you : 13928\n",
      "my : 12490\n",
      "that : 11563\n",
      "in : 11183\n"
     ]
    }
   ],
   "source": [
    "print(\"Most common {0} words\\n====================\".format(n))\n",
    "for (c, w) in mostcommon:\n",
    "    print(\"{0} : {1}\".format(w, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find N least common words in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Same principle as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find N least common words by count\n",
    "n = 10\n",
    "leastcommon = countsRDD \\\n",
    "           .map(lambda (w,c):(c,w)) \\\n",
    "           .sortByKey(ascending=True) \\\n",
    "           .take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least common 10 words\n",
      "=====================\n",
      "aided : 1\n",
      "gag : 1\n",
      "cxsar : 1\n",
      "pretended : 1\n",
      "conjuring : 1\n",
      "offendeth : 1\n",
      "reposeth : 1\n",
      "rupture : 1\n",
      "swoopstake : 1\n",
      "digit : 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Least common {0} words\\n=====================\".format(n))\n",
    "for (c, w) in leastcommon:\n",
    "    print(\"{0} : {1}\".format(w, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[97] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release the cache:\n",
    "countsRDD.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#*That's all, folks!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
